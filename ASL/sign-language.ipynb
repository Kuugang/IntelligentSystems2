{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Phát hiện các landmarks\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m image, results \u001b[38;5;241m=\u001b[39m \u001b[43mmediapipe_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholistic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m extract_keypoints(results)\n\u001b[0;32m     94\u001b[0m keypoints_data\u001b[38;5;241m.\u001b[39mappend(keypoints)\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mmediapipe_detection\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m     15\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)  \u001b[38;5;66;03m# Chuyển đổi từ BGR sang RGB\u001b[39;00m\n\u001b[0;32m     16\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Không thể chỉnh sửa hình ảnh trong quá trình xử lý\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Phân tích hình ảnh\u001b[39;00m\n\u001b[0;32m     18\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Bật lại khả năng chỉnh sửa hình ảnh\u001b[39;00m\n\u001b[0;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)  \u001b[38;5;66;03m# Chuyển lại RGB sang BGR\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kuugang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mediapipe\\python\\solutions\\holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kuugang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Mediapipe model and utilities\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Hàm phát hiện và vẽ các landmarks từ video\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Chuyển đổi từ BGR sang RGB\n",
    "    image.flags.writeable = False  # Không thể chỉnh sửa hình ảnh trong quá trình xử lý\n",
    "    results = model.process(image)  # Phân tích hình ảnh\n",
    "    image.flags.writeable = True  # Bật lại khả năng chỉnh sửa hình ảnh\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Chuyển lại RGB sang BGR\n",
    "    return image, results\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    # Trích xuất các điểm đặc trưng (landmarks) từ các phần khác nhau của cơ thể\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])  # Kết hợp tất cả các điểm vào một mảng\n",
    "\n",
    "# Đọc dữ liệu metadata từ file JSON\n",
    "metadata = {}\n",
    "with open('data/WLASL_v0.3.json', 'r') as file:\n",
    "    metadata = json.load(file)\n",
    "\n",
    "# Tạo label map từ metadata\n",
    "labelMap = {}\n",
    "for i in metadata:\n",
    "    label = i['gloss']\n",
    "    for instance in i['instances']:\n",
    "        video_id = int(instance['video_id'])\n",
    "        frame_start = instance['frame_start']\n",
    "        frame_end = instance['frame_end']\n",
    "        fps = instance['fps']\n",
    "        labelMap[video_id] = [label, frame_start, frame_end, fps]\n",
    "\n",
    "# Tạo thư mục lưu dữ liệu\n",
    "DATA_PATH = 'MP_Data'\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "# Lặp qua từng video trong thư mục dữ liệu\n",
    "video_path = 'data/videos'\n",
    "for video in os.listdir(video_path):\n",
    "    if video.endswith('.mp4'):\n",
    "        video_filename = os.path.basename(video)\n",
    "        video_id = int(os.path.splitext(video_filename)[0])\n",
    "\n",
    "        # Lấy thông tin video từ labelMap\n",
    "        label, start_frame, end_frame, fps = labelMap[video_id]\n",
    "        \n",
    "        # Mở video\n",
    "        cap = cv2.VideoCapture(os.path.join(video_path, video))\n",
    "        cap.set(cv2.CAP_PROP_FPS, fps)\n",
    "\n",
    "        # Khởi tạo mô hình Mediapipe Holistic\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "            # Tạo thư mục cho hành động\n",
    "            action_path = os.path.join(DATA_PATH, label)\n",
    "            if not os.path.exists(action_path):\n",
    "                os.makedirs(action_path)\n",
    "\n",
    "            # Tạo thư mục cho video\n",
    "            video_dir = os.path.join(action_path, str(video_id))\n",
    "            if not os.path.exists(video_dir):\n",
    "                os.makedirs(video_dir)\n",
    "\n",
    "            frame_count = 0\n",
    "            keypoints_data = []\n",
    "\n",
    "            # Đọc và xử lý từng frame\n",
    "            while cap.isOpened():\n",
    "                success, image = cap.read()\n",
    "                if not success:\n",
    "                    break\n",
    "                frame_count += 1\n",
    "\n",
    "                # Nếu frame nằm ngoài khoảng (start_frame, end_frame), bỏ qua\n",
    "                if frame_count < start_frame or (end_frame != -1 and frame_count > end_frame):\n",
    "                    continue\n",
    "\n",
    "                # Phát hiện các landmarks\n",
    "                image, results = mediapipe_detection(image, holistic)\n",
    "                keypoints = extract_keypoints(results)\n",
    "                keypoints_data.append(keypoints)\n",
    "\n",
    "            # Lưu dữ liệu keypoints dưới dạng numpy array\n",
    "            np.save(os.path.join(video_dir, f'{video_id}_keypoints.npy'), np.array(keypoints_data))\n",
    "\n",
    "        # Đóng video\n",
    "        cap.release()\n",
    "\n",
    "print(\"Xử lý xong các video trong bộ dữ liệu!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T15:27:37.499362Z",
     "iopub.status.busy": "2024-11-27T15:27:37.498918Z",
     "iopub.status.idle": "2024-11-27T15:28:06.545268Z",
     "shell.execute_reply": "2024-11-27T15:28:06.543807Z",
     "shell.execute_reply.started": "2024-11-27T15:27:37.499326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T15:34:04.695898Z",
     "iopub.status.busy": "2024-11-27T15:34:04.695447Z",
     "iopub.status.idle": "2024-11-27T15:34:05.229355Z",
     "shell.execute_reply": "2024-11-27T15:34:05.227732Z",
     "shell.execute_reply.started": "2024-11-27T15:34:04.695861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Mediapipe model and utilities\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Hàm phát hiện và vẽ các landmarks từ video\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "# Hàm trích xuất keypoints từ kết quả Mediapipe\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "# Đọc metadata từ JSON\n",
    "metadata = {}\n",
    "with open('/kaggle/input/wlasl-processed/WLASL_v0.3.json', 'r') as file:\n",
    "    metadata = json.load(file)\n",
    "\n",
    "# Tạo label map (chỉ lấy 100 từ đầu tiên, mỗi từ lấy 5 video)\n",
    "labelMap = {}\n",
    "for i in metadata:\n",
    "    label = i['gloss']\n",
    "    for instance in i['instances']:\n",
    "        video_id = int(instance['video_id'])\n",
    "        frame_start = instance['frame_start']\n",
    "        frame_end = instance['frame_end']\n",
    "        fps = instance['fps']\n",
    "        labelMap[video_id] = [label, frame_start, frame_end, fps]\n",
    "\n",
    "# Tạo thư mục lưu dữ liệu\n",
    "DATA_PATH = '/kaggle/working/MP_Data_Frames'\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "# Lặp qua từng video trong thư mục dữ liệu\n",
    "video_path = '/kaggle/input/wlasl-processed/videos'\n",
    "videos_processed = {}\n",
    "\n",
    "for video in os.listdir(video_path):\n",
    "    if video.endswith('.mp4'):\n",
    "        video_id = int(os.path.splitext(video)[0])\n",
    "\n",
    "        if video_id in labelMap:\n",
    "            label, start_frame, end_frame, fps = labelMap[video_id]\n",
    "\n",
    "            if label not in videos_processed:\n",
    "                videos_processed[label] = 0\n",
    "\n",
    "            if videos_processed[label] < 5:\n",
    "                cap = cv2.VideoCapture(os.path.join(video_path, video))\n",
    "                cap.set(cv2.CAP_PROP_FPS, fps)\n",
    "\n",
    "                with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                    # Tạo thư mục cho từ\n",
    "                    action_path = os.path.join(DATA_PATH, label)\n",
    "                    if not os.path.exists(action_path):\n",
    "                        os.makedirs(action_path)\n",
    "\n",
    "                    # Tạo thư mục cho video\n",
    "                    video_dir = os.path.join(action_path, str(video_id))\n",
    "                    if not os.path.exists(video_dir):\n",
    "                        os.makedirs(video_dir)\n",
    "\n",
    "                    frame_count = 0\n",
    "                    while cap.isOpened():\n",
    "                        success, image = cap.read()\n",
    "                        if not success:\n",
    "                            break\n",
    "                        frame_count += 1\n",
    "\n",
    "                        if frame_count < start_frame or (end_frame != -1 and frame_count > end_frame):\n",
    "                            continue\n",
    "\n",
    "                        image, results = mediapipe_detection(image, holistic)\n",
    "                        keypoints = extract_keypoints(results)\n",
    "\n",
    "                        # Lưu mỗi frame vào file riêng biệt\n",
    "                        np.save(os.path.join(video_dir, f'{frame_count}.npy'), keypoints)\n",
    "\n",
    "                    cap.release()\n",
    "                videos_processed[label] += 1\n",
    "\n",
    "print(\"Xử lý hoàn tất!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T14:13:35.694456Z",
     "iopub.status.busy": "2024-11-27T14:13:35.693367Z",
     "iopub.status.idle": "2024-11-27T14:13:58.240279Z",
     "shell.execute_reply": "2024-11-27T14:13:58.238694Z",
     "shell.execute_reply.started": "2024-11-27T14:13:35.694412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r folder.zip /kaggle/working/MP_Data_2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lấy danh sách hành động"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T14:25:14.417394Z",
     "iopub.status.busy": "2024-11-27T14:25:14.416824Z",
     "iopub.status.idle": "2024-11-27T14:25:14.425601Z",
     "shell.execute_reply": "2024-11-27T14:25:14.424343Z",
     "shell.execute_reply.started": "2024-11-27T14:25:14.417349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Đường dẫn tới thư mục cần kiểm tra\n",
    "root_folder = \"/kaggle/working/MP_Data_2000\"\n",
    "\n",
    "# Lấy danh sách các thư mục con trong thư mục gốc\n",
    "actions = [name for name in os.listdir(root_folder) if os.path.isdir(os.path.join(root_folder, name))]\n",
    "\n",
    "# In danh sách các thư mục\n",
    "print(actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T15:20:29.606495Z",
     "iopub.status.busy": "2024-11-27T15:20:29.606063Z",
     "iopub.status.idle": "2024-11-27T15:20:46.131151Z",
     "shell.execute_reply": "2024-11-27T15:20:46.129802Z",
     "shell.execute_reply.started": "2024-11-27T15:20:29.606459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map\n",
    "\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        sequence_path = os.path.join(DATA_PATH, action, str(sequence))\n",
    "        num_frames = len(os.listdir(sequence_path))\n",
    "        \n",
    "        for frame_num in range(num_frames):  # Lấy tất cả các frame có sẵn\n",
    "            res = np.load(os.path.join(sequence_path, \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        \n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "\n",
    "np.array(sequences).shape\n",
    "np.array(labels).shape\n",
    "X = np.array(sequences)\n",
    "X.shape\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit(X_train, y_train, epochs=200, callbacks=[tb_callback])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "res = model.predict(X_test)\n",
    "actions[np.argmax(res[2])]\n",
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save('action.h5')\n",
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')\n",
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "multilabel_confusion_matrix(ytrue, yhat)\n",
    "accuracy_score(ytrue, yhat)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1589971,
     "sourceId": 2632847,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
